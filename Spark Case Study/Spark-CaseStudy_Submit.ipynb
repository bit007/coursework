{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark sesssion\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Case-Study\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data into a dataframe with inferschema.\n",
    "\n",
    "df_raw = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons Number: bigint, Plate ID: string, Registration State: string, Issue Date: timestamp, Violation Code: int, Vehicle Body Type: string, Vehicle Make: string, Violation Precinct: int, Issuer Precinct: int, Violation Time: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the dataframe\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create schema manually, so that we can change the datatypes and name of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the schema.\n",
    "# Reading the data again with pre defined schema to change column names and data-types inferred previously.\n",
    "\n",
    "mySchema = StructType([\n",
    "  StructField(\"Summons_Number\", LongType(), True),\n",
    "  StructField(\"Plate_ID\", StringType(), True),\n",
    "  StructField(\"Reg_State\", StringType(), True),\n",
    "  StructField(\"Issue_Date\", DateType(), True),\n",
    "  StructField(\"Violation_code\", IntegerType(), True),\n",
    "  StructField(\"V_body_Type\", StringType(), True),\n",
    "  StructField(\"V_Make\", StringType(), True),\n",
    "  StructField(\"Violation_precinct\", IntegerType(), True),\n",
    "  StructField(\"Issuer_precinct\", IntegerType(), True),\n",
    "  StructField(\"Violation_time\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Reading the data into a dataframe with user defined schema.\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(mySchema)\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "  .load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n",
    "\n",
    "# Count is same for inferschema and user defined schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: long (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Reg_State: string (nullable = true)\n",
      " |-- Issue_Date: date (nullable = true)\n",
      " |-- Violation_code: integer (nullable = true)\n",
      " |-- V_body_Type: string (nullable = true)\n",
      " |-- V_Make: string (nullable = true)\n",
      " |-- Violation_precinct: integer (nullable = true)\n",
      " |-- Issuer_precinct: integer (nullable = true)\n",
      " |-- Violation_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+---------+----------+--------------+-----------+------+------------------+---------------+--------------+\n",
      "|Summons_Number|Plate_ID|Reg_State|Issue_Date|Violation_code|V_body_Type|V_Make|Violation_precinct|Issuer_precinct|Violation_time|\n",
      "+--------------+--------+---------+----------+--------------+-----------+------+------------------+---------------+--------------+\n",
      "|5092469481    |GZH7067 |NY       |2016-07-10|7             |SUBN       |TOYOT |0                 |0              |0143A         |\n",
      "|5092451658    |GZH7067 |NY       |2016-07-08|7             |SUBN       |TOYOT |0                 |0              |0400P         |\n",
      "|4006265037    |FZX9232 |NY       |2016-08-23|5             |SUBN       |FORD  |0                 |0              |0233P         |\n",
      "|8478629828    |66623ME |NY       |2017-06-14|47            |REFG       |MITSU |14                |14             |1120A         |\n",
      "|7868300310    |37033JV |NY       |2016-11-21|69            |DELV       |INTER |13                |13             |0555P         |\n",
      "+--------------+--------+---------+----------+--------------+-----------+------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the dataframe.\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+-----------------+\n",
      "|summary|      Summons_Number|    Violation_code|Violation_precinct|  Issuer_precinct|\n",
      "+-------+--------------------+------------------+------------------+-----------------+\n",
      "|  count|            10803028|          10803028|          10803028|         10803028|\n",
      "|   mean|6.8174470290656595E9|34.599430455979565| 45.01216260848347|46.82931211508477|\n",
      "| stddev| 2.320233962328229E9|19.359868716323483|40.552560268435805|62.66703577269466|\n",
      "|    min|          1002884949|                 0|                 0|                0|\n",
      "|    max|          8585600044|                99|               933|              997|\n",
      "+-------+--------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysing the data for numeric types\n",
    "df.describe('Summons_Number', 'Violation_code', 'Violation_precinct','Issuer_precinct' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above we can see that Issue_Date lies prior to 2017 as well. We need to use only 2017 data in our analysis.\n",
    "\n",
    "df_2017 = df.filter(year(df.Issue_Date) == 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5431918"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of rows.\n",
    "\n",
    "df_2017.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(min(Issue_Date)=datetime.date(2017, 1, 1), max(Issue_Date)=datetime.date(2017, 12, 31))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking range of issue_date after filtering.\n",
    "df_2017.select(min(\"Issue_Date\"), max(\"Issue_Date\")).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a temp view for our analysis.\n",
    "df_2017.createOrReplaceTempView(\"parking_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Data Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Summons_Number|count|\n",
      "+--------------+-----+\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Summons_Number, count(*) as count FROM parking_data \\\n",
    "          where (Summons_Number is null or lower(Summons_Number)='nan') \\\n",
    "          group by Summons_Number\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Reg_State|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Reg_State, count(*) as count FROM parking_data where (Reg_State is null or lower(Reg_State)='nan') \\\n",
    "          group by Reg_State\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Issue_Date|count|\n",
      "+----------+-----+\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Issue_Date, count(*) as count FROM parking_data where (Issue_Date is null or lower(Issue_Date)='nan') \\\n",
    "          group by Issue_Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Violation_code|count|\n",
      "+--------------+-----+\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Violation_code, count(*) as count FROM parking_data \\\n",
    "          where (Violation_code is null or lower(Violation_code)='nan') \\\n",
    "          group by Violation_code\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|V_body_Type|count|\n",
      "+-----------+-----+\n",
      "|        nan|20201|\n",
      "|        NAN|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT V_body_Type, count(*) as count FROM parking_data where (V_body_Type is null or lower(V_body_Type)='nan') \\\n",
    "          group by V_body_Type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|V_Make|count|\n",
      "+------+-----+\n",
      "|   nan|38509|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT V_Make, count(*) as count FROM parking_data where (V_Make is null or lower(V_Make)='nan') \\\n",
    "          group by V_Make\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|Violation_precinct|count|\n",
      "+------------------+-----+\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Violation_precinct, count(*) as count FROM parking_data \\\n",
    "          where (Violation_precinct is null or lower(Violation_precinct)='nan') \\\n",
    "          group by Violation_precinct\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Issuer_precinct|count|\n",
      "+---------------+-----+\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Issuer_precinct, count(*) as count FROM parking_data \\\n",
    "          where (Issuer_precinct is null or lower(Issuer_precinct)='nan') \\\n",
    "          group by Issuer_precinct\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Violation_time|count|\n",
      "+--------------+-----+\n",
      "|           nan|   16|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Violation_time, count(*) as count FROM parking_data \\\n",
    "          where (Violation_time is null or lower(Violation_time)='nan') \\\n",
    "          group by Violation_time\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are nulls/nans only in the vehicle_body_type and violation_time which we will handle (if required) in later part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We do not need plate Id in our analysis, so dropping these colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Plate_ID']\n",
    "df_2017 = df_2017.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the view definition\n",
    "df_2017.createOrReplaceTempView(\"parking_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find the total number of tickets for the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  count|\n",
      "+-------+\n",
      "|5431918|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2017.agg(countDistinct(col(\"Summons_Number\")).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that summons_number is unique as the number of records and distinct count is the same. \n",
    "So, we can say that total number of tickets in the year = 5431918.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find out the number of unique states from where the cars that got parking tickets came."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|Reg_State|  count|\n",
      "+---------+-------+\n",
      "|       NY|4273951|\n",
      "|       NJ| 475825|\n",
      "|       PA| 140286|\n",
      "|       CT|  70403|\n",
      "|       FL|  69468|\n",
      "|       IN|  45525|\n",
      "|       MA|  38941|\n",
      "|       VA|  34367|\n",
      "|       MD|  30213|\n",
      "|       NC|  27152|\n",
      "|       TX|  18827|\n",
      "|       IL|  18666|\n",
      "|       GA|  17537|\n",
      "|       99|  16055|\n",
      "|       AZ|  12379|\n",
      "|       OH|  12281|\n",
      "|       CA|  12153|\n",
      "|       ME|  10806|\n",
      "|       SC|  10395|\n",
      "|       MN|  10083|\n",
      "|       OK|   9088|\n",
      "|       TN|   8514|\n",
      "|       DE|   7905|\n",
      "|       MI|   7231|\n",
      "|       RI|   5814|\n",
      "|       NH|   4119|\n",
      "|       VT|   3683|\n",
      "|       AL|   3178|\n",
      "|       WA|   3052|\n",
      "|       OR|   2622|\n",
      "|       MO|   2483|\n",
      "|       ON|   2460|\n",
      "|       WI|   2127|\n",
      "|       QB|   1998|\n",
      "|       IA|   1938|\n",
      "|       DC|   1929|\n",
      "|       CO|   1841|\n",
      "|       KY|   1795|\n",
      "|       DP|   1794|\n",
      "|       LA|   1689|\n",
      "|       MS|   1582|\n",
      "|       WV|   1265|\n",
      "|       AR|    994|\n",
      "|       SD|    859|\n",
      "|       NM|    792|\n",
      "|       ID|    763|\n",
      "|       NV|    725|\n",
      "|       KS|    706|\n",
      "|       NE|    704|\n",
      "|       UT|    561|\n",
      "|       MT|    505|\n",
      "|       GV|    348|\n",
      "|       NS|    322|\n",
      "|       AK|    298|\n",
      "|       ND|    254|\n",
      "|       WY|    188|\n",
      "|       HI|    156|\n",
      "|       AB|     79|\n",
      "|       PE|     61|\n",
      "|       NB|     57|\n",
      "|       BC|     54|\n",
      "|       PR|     38|\n",
      "|       MB|     17|\n",
      "|       SK|      9|\n",
      "|       FO|      8|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Reg_State,count(*) as count FROM parking_data group by Reg_State order by count(*) desc').show(100)\n",
    "\n",
    "# We have used show(100) as there are < 100 states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Unique States\n",
    "spark.sql('SELECT distinct Reg_State FROM parking_data').count()\n",
    "\n",
    "# We can see the names of the states in the above sql query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Reg_State = 99 with NY as it has highest records.\n",
    "\n",
    "df_2017_state = df_2017.withColumn(\"Reg_State\", when(df_2017[\"Reg_State\"] == '99', 'NY').otherwise(df[\"Reg_State\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the view definition\n",
    "df_2017_state.createOrReplaceTempView(\"parking_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|Reg_State|  count|\n",
      "+---------+-------+\n",
      "|       NY|4290006|\n",
      "|       NJ| 475825|\n",
      "|       PA| 140286|\n",
      "|       CT|  70403|\n",
      "|       FL|  69468|\n",
      "|       IN|  45525|\n",
      "|       MA|  38941|\n",
      "|       VA|  34367|\n",
      "|       MD|  30213|\n",
      "|       NC|  27152|\n",
      "|       TX|  18827|\n",
      "|       IL|  18666|\n",
      "|       GA|  17537|\n",
      "|       AZ|  12379|\n",
      "|       OH|  12281|\n",
      "|       CA|  12153|\n",
      "|       ME|  10806|\n",
      "|       SC|  10395|\n",
      "|       MN|  10083|\n",
      "|       OK|   9088|\n",
      "|       TN|   8514|\n",
      "|       DE|   7905|\n",
      "|       MI|   7231|\n",
      "|       RI|   5814|\n",
      "|       NH|   4119|\n",
      "|       VT|   3683|\n",
      "|       AL|   3178|\n",
      "|       WA|   3052|\n",
      "|       OR|   2622|\n",
      "|       MO|   2483|\n",
      "|       ON|   2460|\n",
      "|       WI|   2127|\n",
      "|       QB|   1998|\n",
      "|       IA|   1938|\n",
      "|       DC|   1929|\n",
      "|       CO|   1841|\n",
      "|       KY|   1795|\n",
      "|       DP|   1794|\n",
      "|       LA|   1689|\n",
      "|       MS|   1582|\n",
      "|       WV|   1265|\n",
      "|       AR|    994|\n",
      "|       SD|    859|\n",
      "|       NM|    792|\n",
      "|       ID|    763|\n",
      "|       NV|    725|\n",
      "|       KS|    706|\n",
      "|       NE|    704|\n",
      "|       UT|    561|\n",
      "|       MT|    505|\n",
      "|       GV|    348|\n",
      "|       NS|    322|\n",
      "|       AK|    298|\n",
      "|       ND|    254|\n",
      "|       WY|    188|\n",
      "|       HI|    156|\n",
      "|       AB|     79|\n",
      "|       PE|     61|\n",
      "|       NB|     57|\n",
      "|       BC|     54|\n",
      "|       PR|     38|\n",
      "|       MB|     17|\n",
      "|       SK|      9|\n",
      "|       FO|      8|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing statewise counts again. We can see that count for NY has increased \n",
    "spark.sql('SELECT Reg_State,count(*) as count FROM parking_data group by Reg_State order by count(*) desc').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Unique States after changes\n",
    "spark.sql('SELECT distinct Reg_State FROM parking_data').count()\n",
    "\n",
    "# We can see the names of the states in the above sql query.\n",
    "# Count of the state is decreased by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How often does each violation code occur? Display the frequency of the top five violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_code| count|\n",
      "+--------------+------+\n",
      "|            21|768087|\n",
      "|            36|662765|\n",
      "|            38|542079|\n",
      "|            14|476664|\n",
      "|            20|319646|\n",
      "+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 5 violation codes\n",
    "spark.sql('SELECT Violation_code,count(*) as count FROM parking_data group by Violation_code order by count(*) desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? \n",
    "#### a. 'vehicle body type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|V_body_Type|  count|\n",
      "+-----------+-------+\n",
      "|       SUBN|1883954|\n",
      "|       4DSD|1547312|\n",
      "|        VAN| 724029|\n",
      "|       DELV| 358984|\n",
      "|        SDN| 194197|\n",
      "+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 5 vehicle body type \n",
    "spark.sql('SELECT V_body_Type,count(*) as count FROM parking_data group by V_body_Type order by count(*) desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? \n",
    "#### b. 'vehicle make'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|V_Make| count|\n",
      "+------+------+\n",
      "|  FORD|636844|\n",
      "| TOYOT|605291|\n",
      "| HONDA|538884|\n",
      "| NISSA|462017|\n",
      "| CHEVR|356032|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 5 vehicle make type \n",
    "spark.sql('SELECT V_Make,count(*) as count FROM parking_data group by V_Make order by count(*) desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A precinct is a police station that has a certain zone of the city under its command.\n",
    "#### a. 'Violation Precinct' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|Violation_precinct| count|\n",
      "+------------------+------+\n",
      "|                 0|925596|\n",
      "|                19|274445|\n",
      "|                14|203553|\n",
      "|                 1|174702|\n",
      "|                18|169131|\n",
      "|               114|147444|\n",
      "+------------------+------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 5 Violation_precinct type \n",
    "spark.sql('SELECT Violation_precinct,count(*) as count FROM parking_data group by Violation_precinct order by count(*) desc').show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A precinct is a police station that has a certain zone of the city under its command.\n",
    "#### b. 'Issuer Precinct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|Issuer_precinct|  count|\n",
      "+---------------+-------+\n",
      "|              0|1078406|\n",
      "|             19| 266961|\n",
      "|             14| 200495|\n",
      "|              1| 168740|\n",
      "|             18| 162994|\n",
      "|            114| 144054|\n",
      "+---------------+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 5 Issuer_precinct type \n",
    "spark.sql('SELECT Issuer_precinct,count(*) as count FROM parking_data group by Issuer_precinct order by count(*) desc').show(6)\n",
    "\n",
    "# Taking 6 as 0 is errorneous records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations: 'Violation Precinct' & 'Issuer Precinct' \n",
    "1. There are high number of erroneous records in violation precinct and issuer precinct.\n",
    "2. We can see that most parking violations occured in precinct 19, 14, 1, 18, 114 and most tickets were issued in these precincts only. \n",
    "3. It seems that these precincts are the most busiest in the entire New York in terms of traffic violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find the violation code frequencies for three precincts that have issued the most number of tickets. Do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the violation code frequencies for three precincts that have issued the most number of tickets\n",
    "# top 3 precincts are 19, 14, 1 except for 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----+-----------------+\n",
      "|Violation_code|Issuer_precinct|count|fraction_of_total|\n",
      "+--------------+---------------+-----+-----------------+\n",
      "|            46|             19|48445|            18.15|\n",
      "|            38|             19|36386|            13.63|\n",
      "|            37|             19|36056|            13.51|\n",
      "|            14|             19|29797|            11.16|\n",
      "|            21|             19|28415|            10.64|\n",
      "+--------------+---------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for precinct = 19\n",
    "# Total tickets issued by this precinct = 266961\n",
    "spark.sql('SELECT Violation_code, Issuer_precinct, count(*) as count, round(count(*)/266961*100, 2)  as fraction_of_total \\\n",
    "          from parking_data \\\n",
    "           WHERE Issuer_precinct = 19 \\\n",
    "           GROUP BY Violation_code, Issuer_precinct \\\n",
    "           ORDER BY count desc').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----+-----------------+\n",
      "|Violation_code|Issuer_precinct|count|fraction_of_total|\n",
      "+--------------+---------------+-----+-----------------+\n",
      "|            14|             14|45036|            22.46|\n",
      "|            69|             14|30464|            15.19|\n",
      "|            31|             14|22555|            11.25|\n",
      "|            47|             14|18364|             9.16|\n",
      "|            42|             14|10027|              5.0|\n",
      "+--------------+---------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for precinct = 14\n",
    "# Total tickets issued bu this precinct = 200495\n",
    "spark.sql('SELECT Violation_code, Issuer_precinct, count(*) as count, round(count(*)/200495*100, 2)  as fraction_of_total \\\n",
    "          from parking_data \\\n",
    "           WHERE Issuer_precinct = 14 \\\n",
    "           GROUP BY Violation_code, Issuer_precinct \\\n",
    "           ORDER BY count desc').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-----+-----------------+\n",
      "|Violation_code|Issuer_precinct|count|fraction_of_total|\n",
      "+--------------+---------------+-----+-----------------+\n",
      "|            14|              1|38354|            22.73|\n",
      "|            16|              1|19081|            11.31|\n",
      "|            20|              1|15408|             9.13|\n",
      "|            46|              1|12745|             7.55|\n",
      "|            38|              1| 8535|             5.06|\n",
      "+--------------+---------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for precinct = 1\n",
    "# Total tickets issued by this precinct = 168740\n",
    "spark.sql('SELECT Violation_code, Issuer_precinct, count(*) as count, round(count(*)/168740*100, 2)  as fraction_of_total \\\n",
    "          from parking_data \\\n",
    "           WHERE Issuer_precinct = 1 \\\n",
    "           GROUP BY Violation_code, Issuer_precinct \\\n",
    "           ORDER BY count desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations: Violation_code\n",
    "#### a. Do these precinct zones have an exceptionally high frequency of certain violation codes?\n",
    "1. Overall top violation codes for all precincts as seen from query done earlier is (21, 36, 38, 14, 20) in this order desc.\n",
    "2. For precinct 19, top 5 violation codes (46, 38, 37, 14, 21)  in given order accounts for 67.09% of total tickets issues by this precinct. Here 46, 38 and 37, 14 have slightly higher fractions than others.\n",
    "3. For precinct 14, top 5 violation codes (14, 69, 31, 47, 42)  in given order accounts for 63.06% of total tickets issues by this precinct. Here 14, 69 and 31 have slightly higher fractions than others. \n",
    "4. For precinct 1, top 5 violation codes (14, 16, 20, 46, 38)  in given order accounts for 55.78% of total tickets issues by this precinct. Here 14 and 16 have slightly higher fractions than others. \n",
    "    \n",
    "#### b. Are these codes common across precincts? \n",
    "Violation code 14 is in top 5 of all three precincts 19, 14 and 1.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find out the properties of parking violations across different times of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(a). Find a way to deal with missing values, if any.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for nulls\n",
    "spark.sql('SELECT count(*) as count FROM parking_data where Violation_time is null ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----------+--------------+-----------+------+------------------+---------------+--------------+\n",
      "|Summons_Number|Reg_State|Issue_Date|Violation_code|V_body_Type|V_Make|Violation_precinct|Issuer_precinct|Violation_time|\n",
      "+--------------+---------+----------+--------------+-----------+------+------------------+---------------+--------------+\n",
      "+--------------+---------+----------+--------------+-----------+------+------------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using isNull()\n",
    "df_2017_state[df_2017_state.Violation_time.isNull()].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   16|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are no nulls in violation time.\n",
    "# Now we would check for NaN.\n",
    "\n",
    "spark.sql(\"SELECT count(*) as count FROM parking_data where lower(Violation_time) = 'nan' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5431902"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are only 16 such records. So, we can drop them and do the further analysis.\n",
    "\n",
    "parking_date_noNull=df_2017_state.filter(lower(df_2017_state.Violation_time) != 'nan')\n",
    "parking_date_noNull.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the existing view with filtered dataframe\n",
    "\n",
    "parking_date_noNull.createOrReplaceTempView(\"parking_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|min_hour|max_hour|AM_PM|\n",
      "+--------+--------+-----+\n",
      "|       4|       4|    0|\n",
      "|       0|      12|    A|\n",
      "|       0|      87|    P|\n",
      "|       3|      10|     |\n",
      "+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysing for inconsistencies \n",
    "\n",
    "spark.sql(\"SELECT min(cast(substr(Violation_time, 1,2) as int)) as min_hour, \\\n",
    "          max(cast(substr(Violation_time, 1,2) as int)) as max_hour, \\\n",
    "          substr(Violation_time, 5,1) as AM_PM \\\n",
    "          FROM parking_data  group by substr(Violation_time, 5,1) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   63|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the count of in-consistent timestamps\n",
    "\n",
    "spark.sql(\"SELECT count(*) as count FROM parking_data where \\\n",
    "          (substr(Violation_time, 5,1) not in ('A', 'P') \\\n",
    "          or cast(substr(Violation_time, 1,2) as int) > 12 )\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are only 63 such records. So, we can drop them and do the further analysis.\n",
    "\n",
    "df_filtered_timestamp = spark.sql(\"SELECT * FROM parking_data where not \\\n",
    "          (substr(Violation_time, 5,1) not in ('A', 'P') \\\n",
    "          or cast(substr(Violation_time, 1,2) as int) > 12 )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the view definition\n",
    "\n",
    "df_filtered_timestamp.createOrReplaceTempView(\"parking_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  count|\n",
      "+-------+\n",
      "|5431839|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select count(*) as count from parking_data').show()\n",
    "\n",
    "# Only 63 rows are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|min_hour|max_hour|AM_PM|\n",
      "+--------+--------+-----+\n",
      "|       0|      12|    A|\n",
      "|       0|      12|    P|\n",
      "+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking again\n",
    "\n",
    "spark.sql(\"SELECT min(cast(substr(Violation_time, 1,2) as int)) as min_hour, \\\n",
    "          max(cast(substr(Violation_time, 1,2) as int)) as max_hour, \\\n",
    "          substr(Violation_time, 5,1) as AM_PM \\\n",
    "          FROM parking_data  group by substr(Violation_time, 5,1) \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(b). The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating  buckets based on hours\n",
    "# Taking assumption for violation time of format 00xxP as Afternoon as considering it at 12 PM.\n",
    "\n",
    "df_bucketed = spark.sql(\"select parking_data.*, \\\n",
    "                CASE WHEN substr(Violation_time,1,2) between 00 and 03\\\n",
    "                                        AND substr(Violation_time,5,1)='A' THEN 'Late_Night' \\\n",
    "                                       WHEN substr(Violation_time,1,2)=12\\\n",
    "                                        AND substr(Violation_time,5,1)='A' THEN 'Late_Night' \\\n",
    "                                     WHEN substr(Violation_time,1,2) between 04 and 07\\\n",
    "                                        AND substr(Violation_time,5,1)='A' THEN 'Early_Morning'\\\n",
    "                                     WHEN substr(Violation_time,1,2) between 08 and 11\\\n",
    "                                        AND substr(Violation_time,5,1)='A' THEN 'Morning'\\\n",
    "                                     WHEN substr(Violation_time,1,2)= 12\\\n",
    "                                        AND substr(Violation_time,5,1)='P' THEN 'After_Noon'\\\n",
    "                                     WHEN substr(Violation_time,1,2) between 00 and 03\\\n",
    "                                        AND substr(Violation_time,5,1)='P' THEN 'After_Noon'\\\n",
    "                                     WHEN substr(Violation_time,1,2) between 04 and 07\\\n",
    "                                        AND substr(Violation_time,5,1)='P' THEN 'Evening' \\\n",
    "                                     WHEN substr(Violation_time,1,2) between 08 and 11\\\n",
    "                                        AND substr(Violation_time,5,1)='P' THEN 'Night'\\\n",
    "                                     ELSE 'No_time' \\\n",
    "                                        END as Violation_Time_Of_Day from parking_data \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the view\n",
    "df_bucketed.createOrReplaceTempView(\"parking_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(c).  Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations.\n",
    "\n",
    "#### The different time of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Violation_Time_Of_Day|\n",
      "+---------------------+\n",
      "|              Evening|\n",
      "|           After_Noon|\n",
      "|              Morning|\n",
      "|           Late_Night|\n",
      "|        Early_Morning|\n",
      "|                Night|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct Violation_Time_Of_Day FROM parking_data \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(c). Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations.\n",
    "\n",
    "#### For each of these groups, find the three most commonly occurring violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+-----+\n",
      "|Violation_Time_Of_Day|Violation_code|count|\n",
      "+---------------------+--------------+-----+\n",
      "|        Early_Morning|            14|74114|\n",
      "|        Early_Morning|            40|60652|\n",
      "|        Early_Morning|            21|57897|\n",
      "+---------------------+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating new view for analysis\n",
    "# Early_Morning\n",
    "spark.sql('SELECT Violation_Time_Of_Day, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            where Violation_Time_Of_Day=\"Early_Morning\"\\\n",
    "            group by Violation_code, Violation_Time_Of_Day order by Violation_Time_Of_Day, count desc, Violation_code ').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------+\n",
      "|Violation_Time_Of_Day|Violation_code| count|\n",
      "+---------------------+--------------+------+\n",
      "|              Morning|            21|598069|\n",
      "|              Morning|            36|348165|\n",
      "|              Morning|            38|176570|\n",
      "+---------------------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating new view for analysis\n",
    "# Morning\n",
    "spark.sql('SELECT Violation_Time_Of_Day, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            where Violation_Time_Of_Day=\"Morning\"\\\n",
    "            group by Violation_code, Violation_Time_Of_Day order by Violation_Time_Of_Day, count desc, Violation_code ').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------+\n",
      "|Violation_Time_Of_Day|Violation_code| count|\n",
      "+---------------------+--------------+------+\n",
      "|           After_Noon|            36|286284|\n",
      "|           After_Noon|            38|240721|\n",
      "|           After_Noon|            37|167026|\n",
      "+---------------------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating new view for analysis\n",
    "# After_Noon\n",
    "spark.sql('SELECT Violation_Time_Of_Day, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            where Violation_Time_Of_Day=\"After_Noon\"\\\n",
    "            group by Violation_code, Violation_Time_Of_Day order by Violation_Time_Of_Day, count desc, Violation_code ').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------+\n",
      "|Violation_Time_Of_Day|Violation_code| count|\n",
      "+---------------------+--------------+------+\n",
      "|              Evening|            38|102855|\n",
      "|              Evening|            14| 75902|\n",
      "|              Evening|            37| 70345|\n",
      "+---------------------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating new view for analysis\n",
    "# Evening\n",
    "spark.sql('SELECT Violation_Time_Of_Day, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            where Violation_Time_Of_Day=\"Evening\"\\\n",
    "            group by Violation_code, Violation_Time_Of_Day order by Violation_Time_Of_Day, count desc, Violation_code ').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+-----+\n",
      "|Violation_Time_Of_Day|Violation_code|count|\n",
      "+---------------------+--------------+-----+\n",
      "|                Night|             7|26293|\n",
      "|                Night|            40|22337|\n",
      "|                Night|            14|21045|\n",
      "+---------------------+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating new view for analysis\n",
    "# Night\n",
    "spark.sql('SELECT Violation_Time_Of_Day, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            where Violation_Time_Of_Day=\"Night\"\\\n",
    "            group by Violation_code, Violation_Time_Of_Day order by Violation_Time_Of_Day, count desc, Violation_code ').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+-----+\n",
      "|Violation_Time_Of_Day|Violation_code|count|\n",
      "+---------------------+--------------+-----+\n",
      "|           Late_Night|            21|36958|\n",
      "|           Late_Night|            40|25867|\n",
      "|           Late_Night|            78|15528|\n",
      "+---------------------+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating new view for analysis\n",
    "# Late_Night\n",
    "spark.sql('SELECT Violation_Time_Of_Day, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            where Violation_Time_Of_Day=\"Late_Night\"\\\n",
    "            group by Violation_code, Violation_Time_Of_Day order by Violation_Time_Of_Day, count desc, Violation_code ').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(d).  Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|Violation_Time_Of_Day|  count|\n",
      "+---------------------+-------+\n",
      "|              Morning|1122804|\n",
      "|           After_Noon| 601700|\n",
      "|              Evening| 116648|\n",
      "|        Early_Morning|  73952|\n",
      "|           Late_Night|  37270|\n",
      "|                Night|  20531|\n",
      "+---------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most commonly occuring violations for each timed buckets\n",
    "# We know that most commonly occuring violation codes are 21, 26, 38 (from earlier queries/)\n",
    "\n",
    "spark.sql('SELECT Violation_Time_Of_Day,count(*) as count FROM parking_data  \\\n",
    "            WHERE Violation_code in (21,36,38) \\\n",
    "            group by Violation_Time_Of_Day order by count desc ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest number of the violations have happened during the Morning time i.e. between 08:00 am and 11:00 am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------+\n",
      "|Violation_Time_Of_Day|Violation_code| count|\n",
      "+---------------------+--------------+------+\n",
      "|           After_Noon|            36|286284|\n",
      "|           After_Noon|            38|240721|\n",
      "|           After_Noon|            21| 74695|\n",
      "|        Early_Morning|            21| 57897|\n",
      "|        Early_Morning|            36| 14782|\n",
      "|        Early_Morning|            38|  1273|\n",
      "|              Evening|            38|102855|\n",
      "|              Evening|            36| 13534|\n",
      "|              Evening|            21|   259|\n",
      "|           Late_Night|            21| 36958|\n",
      "|           Late_Night|            38|   312|\n",
      "|              Morning|            21|598069|\n",
      "|              Morning|            36|348165|\n",
      "|              Morning|            38|176570|\n",
      "|                Night|            38| 20347|\n",
      "|                Night|            21|   184|\n",
      "+---------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Violation_Time_Of_Day, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            WHERE Violation_code in (21,36,38) \\\n",
    "            group by Violation_Time_Of_Day, Violation_code order by Violation_Time_Of_Day, count desc ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Let’s try and find some seasonality in this data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating  buckets based on months\n",
    "This data is for New York. Hence defining seasons based on following:\n",
    "1. Winter: December, January, February\n",
    "2. Spring: March, April, May\n",
    "3. Summer: June, July, August\n",
    "4. Fall: September, October, November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bucketed = spark.sql(\"select parking_data.*, \\\n",
    "                CASE WHEN substr(substr(Issue_Date,6,7),1,2) between 01 and 02 THEN 'Winter' \\\n",
    "                     WHEN substr(substr(Issue_Date,6,7),1,2) between 03 and 05 THEN 'Spring' \\\n",
    "                    WHEN substr(substr(Issue_Date,6,7),1,2) between 06 and 08  THEN 'Summer' \\\n",
    "                    WHEN substr(substr(Issue_Date,6,7),1,2) between 09 and 11  THEN 'Fall' \\\n",
    "                    WHEN substr(substr(Issue_Date,6,7),1,2) = 12  THEN 'Winter' \\\n",
    "                ELSE 'No_Season' END as Violation_Season from parking_data \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the view\n",
    "df_bucketed.createOrReplaceTempView(\"parking_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(a). First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The different seasons of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Violation_Season|\n",
      "+----------------+\n",
      "|          Spring|\n",
      "|          Summer|\n",
      "|            Fall|\n",
      "|          Winter|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct Violation_Season FROM parking_data \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|Violation_Season|  count|\n",
      "+----------------+-------+\n",
      "|          Spring|2873337|\n",
      "|          Winter|1704669|\n",
      "|          Summer| 852854|\n",
      "|            Fall|    979|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most commonly occuring violations for each season\n",
    "spark.sql('SELECT Violation_Season, count(*) as count FROM parking_data  \\\n",
    "            group by Violation_Season order by count desc ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest number of violations occurs in Spring months followed by Winters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6(b). Then, find the three most common violations for each of these seasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+------+\n",
      "|Violation_Season|Violation_code| count|\n",
      "+----------------+--------------+------+\n",
      "|          Spring|            21|402407|\n",
      "|          Spring|            36|344834|\n",
      "|          Spring|            38|271167|\n",
      "+----------------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most commonly occuring violations for each season\n",
    "## Spring\n",
    "spark.sql('SELECT Violation_Season, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            WHERE Violation_Season=\"Spring\"\\\n",
    "            group by Violation_Season, Violation_code \\\n",
    "          order by Violation_Season, count desc ').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+------+\n",
      "|Violation_Season|Violation_code| count|\n",
      "+----------------+--------------+------+\n",
      "|          Winter|            21|238180|\n",
      "|          Winter|            36|221268|\n",
      "|          Winter|            38|187385|\n",
      "+----------------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most commonly occuring violations for each season\n",
    "## Winter\n",
    "spark.sql('SELECT Violation_Season, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            WHERE Violation_Season=\"Winter\"\\\n",
    "            group by Violation_Season, Violation_code \\\n",
    "          order by Violation_Season, count desc ').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+------+\n",
      "|Violation_Season|Violation_code| count|\n",
      "+----------------+--------------+------+\n",
      "|          Summer|            21|127347|\n",
      "|          Summer|            36| 96663|\n",
      "|          Summer|            38| 83518|\n",
      "+----------------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most commonly occuring violations for each season\n",
    "## Summer\n",
    "spark.sql('SELECT Violation_Season, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            WHERE Violation_Season=\"Summer\"\\\n",
    "            group by Violation_Season, Violation_code \\\n",
    "          order by Violation_Season, count desc ').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-----+\n",
      "|Violation_Season|Violation_code|count|\n",
      "+----------------+--------------+-----+\n",
      "|            Fall|            46|  231|\n",
      "|            Fall|            21|  128|\n",
      "|            Fall|            40|  116|\n",
      "+----------------+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most commonly occuring violations for each season\n",
    "## Fall\n",
    "spark.sql('SELECT Violation_Season, Violation_code, count(*) as count FROM parking_data  \\\n",
    "            WHERE Violation_Season=\"Fall\"\\\n",
    "            group by Violation_Season, Violation_code \\\n",
    "          order by Violation_Season, count desc ').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Violation codes are consistent for Spring, Summer and Winter and are 21, 36, 38 in descending order. Whereas during fall season top violation codes are 46, 21, 40 in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Let’s take an example of estimating this for the three most commonly occurring codes:\n",
    "\n",
    "Then, visit the website:\n",
    "http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page\n",
    "It lists the fines associated with different violation codes. They’re divided into two categories: one for the highest-density locations in the city and the other for the rest of the city. For the sake of simplicity, take the average of the two.\n",
    "Using this information, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7(a). Find the total occurrences of the three most common violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_code| count|\n",
      "+--------------+------+\n",
      "|            21|768062|\n",
      "|            36|662765|\n",
      "|            38|542078|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most commonly occuring violations overall after all above steps.\n",
    "## \n",
    "spark.sql('SELECT Violation_code, count(*) as count FROM parking_data  \\\n",
    "            group by Violation_code \\\n",
    "          order by count desc ').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top three violations are \n",
    "##### 21: Street Cleaning: No parking where parking is not allowed by sign, street marking or traffic control device.\n",
    "##### 36: Exceeding the posted speed limit in or near a designated school zone.\n",
    "##### 38: Failing to show a receipt or tag in the windshield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The respective fines for each of the violations:\n",
    "\n",
    "We are taking average amounts for each of the violation codes based on the above page.\n",
    "\n",
    "21: $55\n",
    "\n",
    "36: $50\n",
    "\n",
    "38: $50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7(b). Find the total amount collected for the three violation codes with the maximum tickets. State the code that has the highest total collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+----------------------------+--------------------------+------------------+\n",
      "|Violation_code| count|Parking_Violation_Definition|Parking_Fine_Per_Violation|Total_Parking_Fine|\n",
      "+--------------+------+----------------------------+--------------------------+------------------+\n",
      "|            21|768062|        Street Cleaning: ...|                       $55|         $42243410|\n",
      "|            36|662765|        Exceeding the pos...|                       $50|         $33138250|\n",
      "|            38|542078|        Failing to show a...|                       $50|         $27103900|\n",
      "+--------------+------+----------------------------+--------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most commonly occuring violations overall\n",
    "## \n",
    "spark.sql('SELECT Violation_code, count, \\\n",
    "                     CASE WHEN Violation_code=21 THEN \"Street Cleaning: No parking where parking is not allowed by sign, street marking or traffic control device.\"\\\n",
    "                           WHEN Violation_code=36 THEN \"Exceeding the posted speed limit in or near a designated school zone.\"\\\n",
    "                           WHEN Violation_code=38 THEN \"Failing to show a receipt or tag in the windshield.\"\\\n",
    "                      END AS Parking_Violation_Definition, \\\n",
    "                      CASE WHEN Violation_code=21 THEN \"$55\"\\\n",
    "                           WHEN Violation_code=36 THEN \"$50\"\\\n",
    "                           WHEN Violation_code=38 THEN \"$50\"\\\n",
    "                      END AS Parking_Fine_Per_Violation, \\\n",
    "                      CASE WHEN Violation_code=21 THEN \"$\"||count*55\\\n",
    "                           WHEN Violation_code=36 THEN \"$\"||count*50\\\n",
    "                           WHEN Violation_code=38 THEN \"$\"||count*50\\\n",
    "                      END AS Total_Parking_Fine \\\n",
    "          FROM (SELECT Violation_code, count(*) as count FROM parking_data  \\\n",
    "                  where Violation_code in (21,36,38) \\\n",
    "            group by Violation_code)  order by Total_Parking_Fine desc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For violation code 21 (Street Cleaning: No parking where parking is not allowed by sign, street marking or traffic control device) is the violation code which has the total collection of $ 42,243,410."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7(c). What can you intuitively infer from these findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference:\n",
    "     1) Most commonly occuring parking violations are 21, 36 and 38.\n",
    "     2) Traffic violations in Spring are highest followed by Winter months.\n",
    "     3) In the morning time i.e. 08 am to 11am most of the traffic violations are happening.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
